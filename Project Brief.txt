Project Brief: Ignition Exchange Resource Change Tracker
I. Project Goal
Develop a containerized, self-contained application that scrapes the Inductive Automation Ignition Exchange website daily, tracks resource changes (new or updated projects), generates a comparison report in Excel format, and provides a web dashboard for monitoring and control.

II. Technical Stack & Deployment
Deployment: The entire application must be runnable via a single command: docker compose up.

Backend/Scraper/Scheduler: Python 3.11+ using:

Scraping: The core logic will reside in exchange_scraper_fixed.py.

Web Framework: Flask (or similar lightweight Python framework) for the API and serving the frontend.

Scheduling: APScheduler or Celery (preferred for robustness) integrated into a dedicated service.

Frontend/Dashboard: Single-page application using plain HTML/CSS (Tailwind preferred for rapid styling) and Vanilla JavaScript.

Data Persistence:

Historical Data: Use a dedicated JSON file (data/past_results_cache.json) to store the complete dataset from the last successful scrape for comparison purposes.

File Storage: A persistent Docker volume must be mounted to store the output XLSX files and the historical cache file.

III. File Structure
The final project structure must be ready for a GitHub repository:

/
â”œâ”€â”€ exchange_scraper_fixed.py
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ webserver.py (Flask app, API endpoints)
â”‚   â”œâ”€â”€ scheduler.py (The scheduling worker process)
â”‚   â””â”€â”€ static/
â”‚       â””â”€â”€ index.html (Web Dashboard)
â”œâ”€â”€ data/ (Mounted Docker volume for persistence)
â”‚   â”œâ”€â”€ past_results_cache.json
â”‚   â””â”€â”€ output/
â”‚       â””â”€â”€ [XLSX Files Here]
â”œâ”€â”€ Dockerfile (For the Python application container)
â””â”€â”€ docker-compose.yml

IV. Core Backend Logic (exchange_scraper_fixed.py and Comparison)
The scraper process must handle three steps: Scrape, Compare, and Save.

Scrape: Execute the scraping logic (assume the provided script returns a list of dictionaries, one per project, with key fields including title, version, and updated_date).

Comparison Logic (Current vs. Past):

Load the data from the previous run (the PastResults) from data/past_results_cache.json.

The Updated Results are generated by identifying any project in the current scrape that is either:

New: The project title did not exist in PastResults.

Updated: The project title exists in PastResults, but the version or updated_date value is newer/different.

Save/Output:

Excel Output: Generate the XLSX file: data/output/Ignition Exchange Resource Results_YYMMDD.xlsx (using the current date). If a file with the same date exists, it must be overwritten.

Excel Sheet Contents:

Sheet 1: Updated Results (Contains only the New and Updated projects).

Sheet 2: CurrentResults (Contains all projects from the current scrape).

Sheet 3: PastResults (Contains all projects from the previous successful scrape, loaded from the cache).

Cache Update: Overwrite data/past_results_cache.json with the complete CurrentResults dataset.

V. Docker Compose Requirements
The docker-compose.yml must define at least two services:

web service (Frontend/API): Exposes a port (e.g., 8080) for the dashboard.

scheduler service (Worker): Responsible for executing the scraping job on a loop.

Both services must share a volume mount for the data/ directory to ensure persistence.

VI. Web Dashboard (Frontend) Requirements
The dashboard (index.html) must implement all features shown in the mockup and communicate with the Flask API:

Dashboard Sections:

Current Status: Display real-time state (Running, Paused, Stopped, Done), progress bar, counts (e.g., 405/506), current project being scraped, elapsed time, and estimated remaining time.

Schedule: Allow the user to set the run interval (in hours) and display the calculated next run time. Must include a [Save] button to persist the schedule to the backend.

Quick Actions: Implement API endpoints for the [â–¶ Run Now], [â¸ Pause], and [â¹ Stop] actions.

Live Activity Log: Display a streaming log of the scraper's activity (e.g., Scraped: Project Name (vX.X.X), Loading: Project Name...). This can be achieved by polling a /api/log endpoint.

Recent Jobs: Display a table of job history, including Date, Duration, Status, Count, and Errors.

File Download Feature:

The [ğŸ“Š View Data] button must lead to a separate view or a modal that lists available Excel files (with dates) from the data/output/ directory, allowing the user to download the files directly. A /api/files endpoint is needed to list available reports.

VII. Implementation Note for Agent
The agent must generate the initial content of the required files, including exchange_scraper_fixed.py, Dockerfile, docker-compose.yml, and all Python/HTML components necessary to implement the full functional stack.